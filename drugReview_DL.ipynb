{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "drugReview_DL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Q0utBCYkY2Qf"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yoh5wHeQXx5",
        "colab_type": "code",
        "outputId": "3a036dfc-052e-4d60-a4f9-e818d9838eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import matplotlib.pyplot as plt  \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "import scipy.optimize\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "import string\n",
        "from nltk.stem.porter import *\n",
        "from nltk import word_tokenize \n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "%matplotlib inline\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YzI3P4mRUgJ",
        "colab_type": "code",
        "outputId": "a5fe7b96-9469-4ac2-a585-41457ab7a801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSLvgKWuTTwm",
        "colab_type": "text"
      },
      "source": [
        "# Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rABv4PNTQawb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "drug = pd.read_csv(path+\"drugsCom_pro.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwNqpejrW3r-",
        "colab_type": "code",
        "outputId": "0ab30795-bc33-46e4-f90f-1faa1301fa5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "drug = drug.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\n",
        "drug.head()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>drugName</th>\n",
              "      <th>condition</th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>usefulCount</th>\n",
              "      <th>pre_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>159862</td>\n",
              "      <td>Bactrim</td>\n",
              "      <td>Urinary Tract Infection</td>\n",
              "      <td>\"Took Bactrim for 3 days. Cleared up the urina...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>July 29, 2013</td>\n",
              "      <td>101</td>\n",
              "      <td>[['took', 'bactrim', 'days', 'cleared', 'urina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>217620</td>\n",
              "      <td>Clarithromycin</td>\n",
              "      <td>Sinusitis</td>\n",
              "      <td>\"I was prescribed this medicine for sinus and ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>December 1, 2009</td>\n",
              "      <td>13</td>\n",
              "      <td>[['prescribed', 'medicine', 'sinus', 'flu', 'l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>85750</td>\n",
              "      <td>Ethinyl estradiol / norgestimate</td>\n",
              "      <td>Birth Control</td>\n",
              "      <td>\"I&amp;#039;ve been on this pill for over three mo...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>November 15, 2013</td>\n",
              "      <td>11</td>\n",
              "      <td>[['pill', 'three', 'months', 'tri', 'spintec',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4852</td>\n",
              "      <td>Belviq</td>\n",
              "      <td>Obesity</td>\n",
              "      <td>\"I tried Belviq for Two weeks great med but no...</td>\n",
              "      <td>6.0</td>\n",
              "      <td>December 9, 2015</td>\n",
              "      <td>10</td>\n",
              "      <td>[['tried', 'belviq', 'two', 'weeks', 'great', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>184456</td>\n",
              "      <td>Hydroxyzine</td>\n",
              "      <td>Anxiety</td>\n",
              "      <td>\"This medicine only stops the physical effects...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>March 27, 2011</td>\n",
              "      <td>53</td>\n",
              "      <td>[['medicine', 'stops', 'physical', 'effects', ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0.1.1  ...                                         pre_review\n",
              "0          159862  ...  [['took', 'bactrim', 'days', 'cleared', 'urina...\n",
              "1          217620  ...  [['prescribed', 'medicine', 'sinus', 'flu', 'l...\n",
              "2           85750  ...  [['pill', 'three', 'months', 'tri', 'spintec',...\n",
              "3            4852  ...  [['tried', 'belviq', 'two', 'weeks', 'great', ...\n",
              "4          184456  ...  [['medicine', 'stops', 'physical', 'effects', ...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfEXHW_74R84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drug['pre_review'] = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRC4BSSFRMJc",
        "colab_type": "code",
        "outputId": "69f5aac0-8b30-4a00-85b1-9f20ad7f3e9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# ## Preprocessing: tokenize and remove stopwords\n",
        "\n",
        "# pro_review = []\n",
        "\n",
        "# for i in range(0, len(drug['review'])):\n",
        "#   # print(drug['review'][i])\n",
        "#   # Cleaing the text\n",
        "#   processed_review = drug['review'][i].lower()\n",
        "#   processed_review = re.sub('[^a-zA-Z]', ' ', processed_review )\n",
        "#   processed_review = re.sub(r'\\s+', ' ', processed_review)\n",
        "\n",
        "#   all_sentences = nltk.sent_tokenize(processed_review)\n",
        "\n",
        "#   all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
        "\n",
        "#   # Removing Stop Words\n",
        "\n",
        "#   for j in range(len(all_words)):\n",
        "#     all_words[j] = [w for w in all_words[j] if w not in stopwords.words('english')]\n",
        "\n",
        "#   drug['pre_review'][i] = all_words\n",
        "#   # print(drug['pre_review'][i])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPh9m63cHzFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drug.to_csv(path+'drugsComTrain_processed.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I32GSFW9Ywtv",
        "colab_type": "code",
        "outputId": "a6be562a-9c07-4fdd-b33b-5254181f3b45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "pre_review = drug['pre_review'].str.lower().str.split()\n",
        "drug['review_length'] = pre_review.apply(len)\n",
        "\n",
        "drug['pre_review'] = drug['pre_review'].str.lower().str.split()\n",
        "\n",
        "drug.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>drugName</th>\n",
              "      <th>condition</th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>usefulCount</th>\n",
              "      <th>pre_review</th>\n",
              "      <th>review_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>159862</td>\n",
              "      <td>Bactrim</td>\n",
              "      <td>Urinary Tract Infection</td>\n",
              "      <td>\"Took Bactrim for 3 days. Cleared up the urina...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>July 29, 2013</td>\n",
              "      <td>101</td>\n",
              "      <td>[[['took',, 'bactrim',, 'days',, 'cleared',, '...</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>217620</td>\n",
              "      <td>Clarithromycin</td>\n",
              "      <td>Sinusitis</td>\n",
              "      <td>\"I was prescribed this medicine for sinus and ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>December 1, 2009</td>\n",
              "      <td>13</td>\n",
              "      <td>[[['prescribed',, 'medicine',, 'sinus',, 'flu'...</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>85750</td>\n",
              "      <td>Ethinyl estradiol / norgestimate</td>\n",
              "      <td>Birth Control</td>\n",
              "      <td>\"I&amp;#039;ve been on this pill for over three mo...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>November 15, 2013</td>\n",
              "      <td>11</td>\n",
              "      <td>[[['pill',, 'three',, 'months',, 'tri',, 'spin...</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4852</td>\n",
              "      <td>Belviq</td>\n",
              "      <td>Obesity</td>\n",
              "      <td>\"I tried Belviq for Two weeks great med but no...</td>\n",
              "      <td>6.0</td>\n",
              "      <td>December 9, 2015</td>\n",
              "      <td>10</td>\n",
              "      <td>[[['tried',, 'belviq',, 'two',, 'weeks',, 'gre...</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>184456</td>\n",
              "      <td>Hydroxyzine</td>\n",
              "      <td>Anxiety</td>\n",
              "      <td>\"This medicine only stops the physical effects...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>March 27, 2011</td>\n",
              "      <td>53</td>\n",
              "      <td>[[['medicine',, 'stops',, 'physical',, 'effect...</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0.1.1  ... review_length\n",
              "0          159862  ...            20\n",
              "1          217620  ...            18\n",
              "2           85750  ...            35\n",
              "3            4852  ...            12\n",
              "4          184456  ...            25\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEufzb1P575r",
        "colab_type": "code",
        "outputId": "1cfe71e9-fa68-44db-a67e-98b83a22c12e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "## Assign label to ratings\n",
        "def get_rating(rating):\n",
        "  if rating < 7:\n",
        "    return 0\n",
        "  # elif rating < 8:\n",
        "  #   return 1\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "drug['pre_rating'] = drug['rating'].apply(get_rating)\n",
        "drug.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>drugName</th>\n",
              "      <th>condition</th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>usefulCount</th>\n",
              "      <th>pre_review</th>\n",
              "      <th>review_length</th>\n",
              "      <th>pre_rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>159862</td>\n",
              "      <td>Bactrim</td>\n",
              "      <td>Urinary Tract Infection</td>\n",
              "      <td>\"Took Bactrim for 3 days. Cleared up the urina...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>July 29, 2013</td>\n",
              "      <td>101</td>\n",
              "      <td>[[['took',, 'bactrim',, 'days',, 'cleared',, '...</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>217620</td>\n",
              "      <td>Clarithromycin</td>\n",
              "      <td>Sinusitis</td>\n",
              "      <td>\"I was prescribed this medicine for sinus and ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>December 1, 2009</td>\n",
              "      <td>13</td>\n",
              "      <td>[[['prescribed',, 'medicine',, 'sinus',, 'flu'...</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>85750</td>\n",
              "      <td>Ethinyl estradiol / norgestimate</td>\n",
              "      <td>Birth Control</td>\n",
              "      <td>\"I&amp;#039;ve been on this pill for over three mo...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>November 15, 2013</td>\n",
              "      <td>11</td>\n",
              "      <td>[[['pill',, 'three',, 'months',, 'tri',, 'spin...</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4852</td>\n",
              "      <td>Belviq</td>\n",
              "      <td>Obesity</td>\n",
              "      <td>\"I tried Belviq for Two weeks great med but no...</td>\n",
              "      <td>6.0</td>\n",
              "      <td>December 9, 2015</td>\n",
              "      <td>10</td>\n",
              "      <td>[[['tried',, 'belviq',, 'two',, 'weeks',, 'gre...</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>184456</td>\n",
              "      <td>Hydroxyzine</td>\n",
              "      <td>Anxiety</td>\n",
              "      <td>\"This medicine only stops the physical effects...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>March 27, 2011</td>\n",
              "      <td>53</td>\n",
              "      <td>[[['medicine',, 'stops',, 'physical',, 'effect...</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0.1.1                          drugName  ... review_length pre_rating\n",
              "0          159862                           Bactrim  ...            20          0\n",
              "1          217620                    Clarithromycin  ...            18          0\n",
              "2           85750  Ethinyl estradiol / norgestimate  ...            35          0\n",
              "3            4852                            Belviq  ...            12          0\n",
              "4          184456                       Hydroxyzine  ...            25          0\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_tjxBSxMF-Q",
        "colab_type": "code",
        "outputId": "f841fbfe-4a6b-4330-8175-949f57df842b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "drug.groupby(['rating']).count()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0.1.1</th>\n",
              "      <th>drugName</th>\n",
              "      <th>condition</th>\n",
              "      <th>review</th>\n",
              "      <th>date</th>\n",
              "      <th>usefulCount</th>\n",
              "      <th>pre_review</th>\n",
              "      <th>review_length</th>\n",
              "      <th>pre_rating</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rating</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>9956</td>\n",
              "      <td>9956</td>\n",
              "      <td>9902</td>\n",
              "      <td>9956</td>\n",
              "      <td>9956</td>\n",
              "      <td>9956</td>\n",
              "      <td>9956</td>\n",
              "      <td>9956</td>\n",
              "      <td>9956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.0</th>\n",
              "      <td>3168</td>\n",
              "      <td>3168</td>\n",
              "      <td>3145</td>\n",
              "      <td>3168</td>\n",
              "      <td>3168</td>\n",
              "      <td>3168</td>\n",
              "      <td>3168</td>\n",
              "      <td>3168</td>\n",
              "      <td>3168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.0</th>\n",
              "      <td>2993</td>\n",
              "      <td>2993</td>\n",
              "      <td>2967</td>\n",
              "      <td>2993</td>\n",
              "      <td>2993</td>\n",
              "      <td>2993</td>\n",
              "      <td>2993</td>\n",
              "      <td>2993</td>\n",
              "      <td>2993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.0</th>\n",
              "      <td>2315</td>\n",
              "      <td>2315</td>\n",
              "      <td>2304</td>\n",
              "      <td>2315</td>\n",
              "      <td>2315</td>\n",
              "      <td>2315</td>\n",
              "      <td>2315</td>\n",
              "      <td>2315</td>\n",
              "      <td>2315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5.0</th>\n",
              "      <td>3680</td>\n",
              "      <td>3680</td>\n",
              "      <td>3665</td>\n",
              "      <td>3680</td>\n",
              "      <td>3680</td>\n",
              "      <td>3680</td>\n",
              "      <td>3680</td>\n",
              "      <td>3680</td>\n",
              "      <td>3680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6.0</th>\n",
              "      <td>2888</td>\n",
              "      <td>2888</td>\n",
              "      <td>2870</td>\n",
              "      <td>2888</td>\n",
              "      <td>2888</td>\n",
              "      <td>2888</td>\n",
              "      <td>2888</td>\n",
              "      <td>2888</td>\n",
              "      <td>2888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7.0</th>\n",
              "      <td>2173</td>\n",
              "      <td>2173</td>\n",
              "      <td>2160</td>\n",
              "      <td>2173</td>\n",
              "      <td>2173</td>\n",
              "      <td>2173</td>\n",
              "      <td>2173</td>\n",
              "      <td>2173</td>\n",
              "      <td>2173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8.0</th>\n",
              "      <td>4547</td>\n",
              "      <td>4547</td>\n",
              "      <td>4523</td>\n",
              "      <td>4547</td>\n",
              "      <td>4547</td>\n",
              "      <td>4547</td>\n",
              "      <td>4547</td>\n",
              "      <td>4547</td>\n",
              "      <td>4547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9.0</th>\n",
              "      <td>6415</td>\n",
              "      <td>6415</td>\n",
              "      <td>6384</td>\n",
              "      <td>6415</td>\n",
              "      <td>6415</td>\n",
              "      <td>6415</td>\n",
              "      <td>6415</td>\n",
              "      <td>6415</td>\n",
              "      <td>6415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10.0</th>\n",
              "      <td>11865</td>\n",
              "      <td>11865</td>\n",
              "      <td>11805</td>\n",
              "      <td>11865</td>\n",
              "      <td>11865</td>\n",
              "      <td>11865</td>\n",
              "      <td>11865</td>\n",
              "      <td>11865</td>\n",
              "      <td>11865</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0.1.1  drugName  ...  review_length  pre_rating\n",
              "rating                            ...                           \n",
              "1.0               9956      9956  ...           9956        9956\n",
              "2.0               3168      3168  ...           3168        3168\n",
              "3.0               2993      2993  ...           2993        2993\n",
              "4.0               2315      2315  ...           2315        2315\n",
              "5.0               3680      3680  ...           3680        3680\n",
              "6.0               2888      2888  ...           2888        2888\n",
              "7.0               2173      2173  ...           2173        2173\n",
              "8.0               4547      4547  ...           4547        4547\n",
              "9.0               6415      6415  ...           6415        6415\n",
              "10.0             11865     11865  ...          11865       11865\n",
              "\n",
              "[10 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoP3Wp6INztS",
        "colab_type": "code",
        "outputId": "a509e70b-0e0b-4887-9a41-c1efa28aae60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "select_indices = list(np.where(drug['review_length'] == 937)[0])\n",
        "te = drug.iloc[select_indices]['review']\n",
        "drug['review'][8120]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Two and a half months ago I was prescribed Venlafaxine to help prevent chronic migraines.\\r\\nIt did help the migraines (reduced them by almost half), but with it came a host of side effects that were far worse than the problem I was trying to get rid of.\\r\\nHaving now come off of the stuff, I would not recommend anyone ever use Venlafaxine unless they suffer from extreme / suicidal depression. I mean extreme in the most emphatic sense of the word. \\r\\nBefore trying Venlafaxine, I was a writer. While on Venlafaxine, I could barely write or speak or communicate at all. More than that, I just didn&#039;t want to. Not normal for a usually outgoing extrovert.\\r\\nNow, I&#039;m beginning to write again - but my ability to speak and converse with others has deteriorated by about 95%. Writing these words is taking forever; keeping up in conversation with even one person is impossible, and I barely see the point of trying either. On Venlafaxine, words pretty much left me - my conversational vocabulary  was whittled down to the following:\\r\\n&quot;Mmm&quot; for yes; a sharp and clipped &quot;Mm&quot; for &quot;No&quot;\\r\\n&quot;Okay.&quot;\\r\\n&quot;Really?&quot;\\r\\n&quot;Oh right.&quot;\\r\\n&quot;Cool.&quot;\\r\\n&quot;That sucks&quot;\\r\\nAt the moment, I&#039;m a week into withdrawal, and I have to try extremely hard just to make the most mundane small talk. Last night I went to a party with some close friends, cheesy &#039;90s music, and a barbecue. About half of them are academics, and I couldn&#039;t keep up with conversations I normally would be able to; the other half like to dance and party, but I felt glued to my seat (as well as practically mute) and eventually walked off and found a quiet sofa to lie down on.\\r\\nI have never felt so isolated and lonely in my life. Thanks Venlafaxine.\\r\\nIf you consider yourself a social, creative, and curious person, DO NOT TAKE EVEN ONE DOSE OF VENLAFAXINE. Unless of course you are extremely extremely depressed. I&#039;ve suffered from depression in the past, but only mildly. Venlafaxine has shown me what true depression feels like.\\r\\nAs for the other side effects:\\r\\n- About two days&#039; worth of total joyful euphoria during the first 48 hours of taking Venlafaxine. Anxiety of all kinds evaporated. This felt amazing - although I can&#039;t remember what it felt like now. At the time, this extremely brief effect was powerful enough to make me feel that pushing on with Venlafaxine would be a Good Idea.\\r\\n- After that, I started feeling very sleepy. All the time. I slept more. No euphoria; more anxiety.\\r\\n- A week later, the night sweats started. My room was pretty cold (it was only spring in the UK), but I sweated more than I ever have on any tropical holiday. This meant that despite being extremely sleepy, I couldn&#039;t sleep.\\r\\nSince the side effects were supposed to wear off after six weeks or so, I stuck with Venlafaxine for six weeks. Over that time I became so sleep deprived that I lived in a state of permanent exhaustion.\\r\\nEnter the writer&#039;s worst enemy: Brain fog. I spent entire days so braindead that the most challenging thing I could manage was staring blankly at a wall - or lifting my phone to read text messages and attempt to learn something from articles about Venlafaxine. I&#039;ve read the same articles countless times, but nothing sank in; every time I read the same article it felt like I was reading it for the first time. I recognised the layout and design of each webpage, and that way I was able to realise I&#039;d read it before, but the actual text went in one eye and out the other.\\r\\nIn conversation, I had a brain like a sieve. Words went in one ear and out the other. Normally, I could pump out a lot of writing on a regular basis; on Venlafaxine, I wrote a small and frankly pitiful handful of short and uninspired pieces, and that was it. If you love being productive and creative, do not go near Venlafaxine.\\r\\nThe above was my life for six weeks - and that was enough. I did see family and friends while on Venlafaxine, but I constantly forgot what was going on and must have seemed scatterbrained or borderline retarded at times. Although I did explain to people what was going on with my medication, it just seemed to make people uncomfortable - and I&#039;ve now become the butt of a lot of jokes relating to my consistent uselessness at everything from chatting to party games and my inability to work or do anything productive. Someone even compared me to Lenny from Of Mice And Men and asked when my family were going to take me into the back garden and shoot me in the head rather than continue to care for me. All of this contributed to the worst feeling of loneliness and isolation I have ever experienced in my life.\\r\\nVenlafaxine not only trapped me inside my head - it also emptied my head of anything worth remembering, and left me barely able to learn new things. \\r\\nI&#039;d sit around tables with people and realise I couldn&#039;t remember the names of people I&#039;d know for years - or even family members. \\r\\nI&#039;d hear a song playing and say &quot;Hey - what&#039;s the name of this dance?&quot;\\r\\nAny sentence longer than a few words requires a minute or so to compose when written. Spoken out loud? Forget about it. When I try to speak, I sound like the Goon from Popeye.\\r\\nAfter six weeks of hell, my doctor and I agreed to taper off my 75mg daily dose. This process lasted a month, not following a particular schedule (which might have been a better idea), and was mostly side effect free until I came down to 18.25mg a day - one half of one 37.5mg tablet.\\r\\nAfter three days of that, it really hit the fan. I got up one day, pottered around a bit, and suddenly decided that I wanted to kill myself. Literally out of nowhere.\\r\\nFortunately I was of sound enough mind to call the Samaritans, who recommended I call the emergency services, who sent an ambulance to take me to hospital. For suicidal thoughts. \\r\\nTo be honest, I&#039;d rather have gone to Disneyland.\\r\\nOnce I was in A&amp;E, I got to wait for several hours, just stewing in a room with a sofa and not much else. Then I met the most useless doctor of all time. After I refused his suggestion that I not only go back on Venlafaxine, but *try a higher dose* than the one that got me into this mess in the first place, he said there wasn&#039;t anything else they could do - and handed me a little leaflet for a local mental health charity meetup that happens every so often.\\r\\nBy this point I&#039;d decided that suicide would not be the best option - and nor would going on even more Venlafaxine than before. I also binned the leaflet on my way out, determined to keep going and just deal with the withdrawal.\\r\\nSince that time, my intelligence level has plummeted to the point of being humiliating. But that&#039;s not even the worst of it.\\r\\nAs I mentioned before, I started taking Venlafaxine for migraines. Now that I&#039;m not taking it, they&#039;re coming back again - but they&#039;ve also changed.\\r\\nA few days ago, days before the party I should probably have stayed home for only I couldn&#039;t stand sitting at home any longer so I went and ended up alone in a darkened room and felt more depressed than I have in my entire life, I was at home when I collapsed. The right side of my body gave way, I hit the wall, and fell on the floor where I lay frozen for God knows how long. Then when I did get up, I realised I couldn&#039;t speak at all, the right side of my body was almost paralysed, and the right side of my head was numb on the outside, and in agony on the inside.\\r\\nAfter calling the emergency services again, I was taken through the standard questions I guess they ask everyone when they think the person in question is having a stroke. Like the suicidal thoughts, stroke symptoms were a new experience for me. Thanks, Venlafaxine.\\r\\nWhen the ambulance arrived, they were able to reassure me that I wasn&#039;t having a stroke, as one side of my face wasn&#039;t drooping. This was good news - but since they couldn&#039;t explain what was actually going on, I was taking to hospital for a brain scan and blood tests and a meeting with a doctor who told me I&#039;d had a right-sided hemiplegic migraine. I&#039;d never had one before, and didn&#039;t know they existed until then; my migraines have always felt like my head is both in a vice and exploding at the same time, and I get them across my whole head, not just one side.\\r\\nAt this point, I&#039;m determined to continue fighting the withdrawal symptoms. The only thing that&#039;s really helped me in doing this was the discovery of a cognitive distortion called &quot;emotional reasoning&quot;. This involves assuming that because you feel bad, things must actually be bad.\\r\\nObviously emotional reasoning (which is worth Googling as it&#039;s quite an in depth subject, or at least feels like it given that my brain&#039;s been hopefully temporarily damaged by Venlafaxine) is pretty common in depression. It&#039;s also been my brain&#039;s default way of operating possibly for as long as I can remember, but definitely since I started taking Venlafaxine. The emotional and physical and psychological rollercoaster Venlafaxine puts you through is utterly exhausting - and while it&#039;s throwing you all over the place and especially during withdrawal it&#039;s tough to keep in mind that a lot of the negative thoughts your mind throws up are going to be based on how you feel (i.e. the levels and mix of different chemicals in your brain), NOT on any Real Life Stuff. You&#039;ll most likely unintentionally filter out all the good stuff in your brain and only remember the darkest and worst things you possibly can, and attach all kinds of apparently logical arguments to make a case against yourself / a case that argues that you and your life are awful.\\r\\nLooking back on this experience, my thoughts were similar to one of those films you see advertised as &quot;...based on a true story&quot;. Works of fiction based very loosely on facts.\\r\\nRather than a film you&#039;d give two stars and never watch again, emotional reasoning&#039;s end product is a lie - not to mention the most toxic thoughts a human being can think. Depression can definitely make people tell these lies to themselves - but Venlafaxine made my brain malfunction so badly that putting together an apparently sensible argument for any depressive thoughts suddenly seemed like the most obvious thing in the world one day.\\r\\nIf I hadn&#039;t had those thoughts, I would&#039;ve just made lunch.\\r\\nSo that&#039;s about it for now - if I don&#039;t update this story in the future, assume I got better and decided to never revisit this page again, preferring to leave Venlafaxine and its horrific toxicity behind me. Good luck with your own journey :)\"'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-INNr6qd3Rxm",
        "colab_type": "code",
        "outputId": "1dc1fce3-be22-4738-a6b6-51fb3866d761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "drug.rating.value_counts().plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f595fd3b9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmIAAAEECAYAAACLE1g8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Dj9tljxYh0U",
        "colab_type": "text"
      },
      "source": [
        "# WordEmbedding and Feature Extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3IfUK2-Yxe0",
        "colab_type": "text"
      },
      "source": [
        "## Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl0N4Bo_Y0uW",
        "colab_type": "code",
        "outputId": "ce81faa3-1edb-4314-b635-39d1cfb669e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "info = api.info()  # show info about available models/datasets\n",
        "embedding_model = api.load(\"glove-twitter-50\")  # download the model and return as object ready for use\n",
        "# embedding_model.most_similar(\"cat\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0utBCYkY2Qf",
        "colab_type": "text"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90d-6RQVY4jL",
        "colab_type": "code",
        "outputId": "89f5948c-dbf6-4cb8-f4e3-ec4dc35ffb08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import nltk\n",
        "import sys\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import treebank\n",
        "from nltk import word_tokenize\n",
        "from nltk.tag import hmm\n",
        "\n",
        "brown_tagged_sents = brown.tagged_sents()\n",
        "\n",
        "# Import HMM module\n",
        "\n",
        "# Setup a trainer with default(None) values\n",
        "# And train with the data\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "pos_tagger = trainer.train_supervised(brown_tagged_sents)\n",
        "\n",
        "print (pos_tagger)\n",
        "# Prints the basic data about the tagger\n",
        "\n",
        "tokens = word_tokenize(\"This race is awesome , I want to race too ! What do you think ?\")\n",
        "print(pos_tagger.tag(tokens))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<HiddenMarkovModelTagger 472 states and 56057 output symbols>\n",
            "[('This', 'DT'), ('race', 'NN'), ('is', 'BEZ'), ('awesome', 'JJ'), (',', ','), ('I', 'PPSS'), ('want', 'VB'), ('to', 'TO'), ('race', 'VB'), ('too', 'RB'), ('!', 'AT'), ('What', 'AT'), ('do', 'AT'), ('you', 'AT'), ('think', 'AT'), ('?', 'AT')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YrWsLc45-__",
        "colab_type": "text"
      },
      "source": [
        "## Lexicon Sentiment Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JE_QnvH6EgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob\n",
        "statement = \"expensive tablet\"\n",
        "sentiment1 = TextBlob(statement).sentiment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLXaT-2s6Lif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def senti(x):\n",
        "    return TextBlob(x).sentiment[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHhmhIdwY9sH",
        "colab_type": "text"
      },
      "source": [
        "# Generate Input For Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4uOan3tZDVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#a dictionary with review id as key, and all the words as value\n",
        "id_review_dict = {}\n",
        "\n",
        "for index, row in drug.iterrows():\n",
        "  review_id = row['Unnamed: 0.1.1']\n",
        "  review_content = row['pre_review']\n",
        "  id_review_dict[review_id] = review_content\n",
        "\n",
        "# id_review_dict[184456]  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3B2DbY2RkyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = []\n",
        "for word in (id_review_dict[184456]):\n",
        "  word = re.sub('[^a-zA-Z]', ' ', word).strip()\n",
        "  temp.append(embedding_model[word])\n",
        "\n",
        "temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc3Ho37Rh5_2",
        "colab_type": "code",
        "outputId": "a730eb19-53c8-468f-8b33-2b4a7418057b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#count the maximum number of words in a single doducment\n",
        "max_input_length = max(drug['review_length'])\n",
        "print('Max input length is: ', max_input_length)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max input length is:  937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKgEZKlh6yo7",
        "colab_type": "code",
        "outputId": "5506d637-14bd-4c29-ee6a-9995cf1a94ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#dictionary with review id as key and the word sentiment score of the whole review.\n",
        "review_lexicon_dict = {}\n",
        "\n",
        "for key in id_review_dict:\n",
        "  str1 = ''.join(id_review_dict[key])\n",
        "  review_lexicon_dict[key] = senti(str1)\n",
        "\n",
        "len(review_lexicon_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjqDGxqw8Mvs",
        "colab_type": "code",
        "outputId": "ffcae22e-a663-4bcf-c0bd-c4bbf2c6decd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "review_lexicon_dict[217620]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17142857142857143"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFWXo4bCliEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dictionary with review id as key and the word embeddings of each words in it as value.\n",
        "review_embedding_dict = {}\n",
        "\n",
        "for key in id_review_dict:\n",
        "  temp_embedding = []\n",
        "  for word in id_review_dict[key]:\n",
        "    word = re.sub('[^a-zA-Z]', ' ', word).strip()\n",
        "    try:\n",
        "      temp_embedding.append(embedding_model[word])\n",
        "    except:\n",
        "      temp_embedding.append(np.zeros(50))\n",
        "\n",
        "  temp_embedding.append(np.full((50), review_lexicon_dict[key]))\n",
        "\n",
        "  review_embedding_dict[key] = temp_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8YROvlyzSit",
        "colab_type": "code",
        "outputId": "9e13de2a-ff46-4d53-cbe2-0cdebf9b38d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(review_embedding_dict[217620])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d-qwns-qnAe",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bLPcPQoNxdc",
        "colab_type": "text"
      },
      "source": [
        "## SGDClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyXkDdlRQj1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(drug['review'], drug['pre_rating'], test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJje7BA5wQku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Using Linear SVM\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "sgd = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                # ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=3000, tol=None)),\n",
        "                ('clf', SGDClassifier(loss='hinge', random_state=42, tol=None)),\n",
        "               ])\n",
        "\n",
        "parameteres = {'clf__alpha':[1e-3,1e-2,1e-1,1,10], 'clf__max_iter':[1000,2000,3000], 'clf__penalty':['l1','l2']}\n",
        "\n",
        "grid = GridSearchCV(sgd, param_grid=parameteres)\n",
        "# sgd.fit(X_train, y_train)\n",
        "\n",
        "# y_pred = sgd.predict(X_test)\n",
        "\n",
        "# print('accuracy %s' % accuracy_score(y_pred, y_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRX11rr02833",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "989c422a-a18c-45dc-de33-b9e32b86d8bd"
      },
      "source": [
        "grid.fit(X_train, y_train)\n",
        "print (\"score = %3.2f\" %(grid.score(X_test,y_test)))\n",
        "print (grid.best_params_)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "score = 0.78\n",
            "{'clf__alpha': 0.001, 'clf__max_iter': 1000, 'clf__penalty': 'l2'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYMA-ejVuaLP",
        "colab_type": "text"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I5V-tEhBFMr",
        "colab_type": "code",
        "outputId": "3781b5cd-7935-4aee-dda2-c7aa4c8df0ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClXGTHsRXnNm",
        "colab_type": "code",
        "outputId": "af717762-1687-4c24-9d30-2b0d3c456bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X = list(review_embedding_dict.values())\n",
        "y = drug['pre_rating']\n",
        "\n",
        "print(len(X))\n",
        "print(len(y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tjqp5Vw-ZGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y = drug['rating']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBXwfpCvD9sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp_USd2MGHS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_input_length+1)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_input_length+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwYOnNoEA7AR",
        "colab_type": "code",
        "outputId": "f6e7e69a-dcef-4fe5-ac3e-4810f7f1f0b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "embedding_vecor_length = 50\n",
        "data_size = 50000\n",
        "max_input = max_input_length+1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(100,input_shape=(max_input, embedding_vecor_length)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(4, activation='sigmoid'))\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_2 (LSTM)                (None, 100)               60400     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 404       \n",
            "=================================================================\n",
            "Total params: 60,804\n",
            "Trainable params: 60,804\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG4FjI3yVRJC",
        "colab_type": "code",
        "outputId": "7abd1b60-f6b9-42c4-b1b7-22a353b07e73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)\n",
        "\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "40000/40000 [==============================] - 718s 18ms/step - loss: 1.0159 - acc: 0.4919 - val_loss: 0.9901 - val_acc: 0.5280\n",
            "Epoch 2/10\n",
            "40000/40000 [==============================] - 641s 16ms/step - loss: 0.9358 - acc: 0.5768 - val_loss: 0.9175 - val_acc: 0.5881\n",
            "Epoch 3/10\n",
            "40000/40000 [==============================] - 575s 14ms/step - loss: 0.8955 - acc: 0.6029 - val_loss: 0.9047 - val_acc: 0.5910\n",
            "Epoch 4/10\n",
            "40000/40000 [==============================] - 575s 14ms/step - loss: 0.8625 - acc: 0.6189 - val_loss: 0.8629 - val_acc: 0.6219\n",
            "Epoch 5/10\n",
            "40000/40000 [==============================] - 575s 14ms/step - loss: 0.8360 - acc: 0.6357 - val_loss: 0.8459 - val_acc: 0.6358\n",
            "Epoch 6/10\n",
            "40000/40000 [==============================] - 572s 14ms/step - loss: 0.8122 - acc: 0.6482 - val_loss: 0.8373 - val_acc: 0.6409\n",
            "Epoch 7/10\n",
            "40000/40000 [==============================] - 575s 14ms/step - loss: 0.7892 - acc: 0.6603 - val_loss: 0.8253 - val_acc: 0.6463\n",
            "Epoch 8/10\n",
            "40000/40000 [==============================] - 569s 14ms/step - loss: 0.7694 - acc: 0.6705 - val_loss: 0.8168 - val_acc: 0.6499\n",
            "Epoch 9/10\n",
            "40000/40000 [==============================] - 567s 14ms/step - loss: 0.7528 - acc: 0.6791 - val_loss: 0.8193 - val_acc: 0.6485\n",
            "Epoch 10/10\n",
            "40000/40000 [==============================] - 567s 14ms/step - loss: 0.7339 - acc: 0.6894 - val_loss: 0.8433 - val_acc: 0.6437\n",
            "Accuracy: 64.37%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_iEFWQO5Tim",
        "colab_type": "code",
        "outputId": "c9c1a41c-f0b4-4040-c879-e56e6ce22a23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "embedding_vecor_length = 50\n",
        "data_size = 50000\n",
        "max_input = max_input_length+1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=50, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(100,input_shape=(max_input, embedding_vecor_length)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train.values, validation_data=(X_test, y_test), epochs=20, batch_size=64)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test.values, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "40000/40000 [==============================] - 456s 11ms/step - loss: 0.6227 - acc: 0.6516 - val_loss: 0.5855 - val_acc: 0.6852\n",
            "Epoch 2/20\n",
            "40000/40000 [==============================] - 454s 11ms/step - loss: 0.5674 - acc: 0.7033 - val_loss: 0.5532 - val_acc: 0.7166\n",
            "Epoch 3/20\n",
            "40000/40000 [==============================] - 448s 11ms/step - loss: 0.5371 - acc: 0.7274 - val_loss: 0.5419 - val_acc: 0.7226\n",
            "Epoch 4/20\n",
            "40000/40000 [==============================] - 447s 11ms/step - loss: 0.5141 - acc: 0.7464 - val_loss: 0.5389 - val_acc: 0.7263\n",
            "Epoch 5/20\n",
            "40000/40000 [==============================] - 448s 11ms/step - loss: 0.4863 - acc: 0.7651 - val_loss: 0.5459 - val_acc: 0.7242\n",
            "Epoch 6/20\n",
            "40000/40000 [==============================] - 450s 11ms/step - loss: 0.4697 - acc: 0.7749 - val_loss: 0.5243 - val_acc: 0.7417\n",
            "Epoch 7/20\n",
            "40000/40000 [==============================] - 451s 11ms/step - loss: 0.4501 - acc: 0.7891 - val_loss: 0.5321 - val_acc: 0.7457\n",
            "Epoch 8/20\n",
            "40000/40000 [==============================] - 452s 11ms/step - loss: 0.4368 - acc: 0.7966 - val_loss: 0.5268 - val_acc: 0.7405\n",
            "Epoch 9/20\n",
            "40000/40000 [==============================] - 451s 11ms/step - loss: 0.4642 - acc: 0.7800 - val_loss: 0.5283 - val_acc: 0.7412\n",
            "Epoch 10/20\n",
            "40000/40000 [==============================] - 449s 11ms/step - loss: 0.4290 - acc: 0.8010 - val_loss: 0.5377 - val_acc: 0.7464\n",
            "Epoch 11/20\n",
            "40000/40000 [==============================] - 451s 11ms/step - loss: 0.4144 - acc: 0.8109 - val_loss: 0.5293 - val_acc: 0.7417\n",
            "Epoch 12/20\n",
            "40000/40000 [==============================] - 460s 12ms/step - loss: 0.4017 - acc: 0.8174 - val_loss: 0.5390 - val_acc: 0.7447\n",
            "Epoch 13/20\n",
            "40000/40000 [==============================] - 458s 11ms/step - loss: 0.3873 - acc: 0.8262 - val_loss: 0.5382 - val_acc: 0.7494\n",
            "Epoch 14/20\n",
            "40000/40000 [==============================] - 457s 11ms/step - loss: 0.3801 - acc: 0.8321 - val_loss: 0.5577 - val_acc: 0.7373\n",
            "Epoch 15/20\n",
            "40000/40000 [==============================] - 454s 11ms/step - loss: 0.3718 - acc: 0.8350 - val_loss: 0.5494 - val_acc: 0.7441\n",
            "Epoch 16/20\n",
            "40000/40000 [==============================] - 463s 12ms/step - loss: 0.3656 - acc: 0.8371 - val_loss: 0.5739 - val_acc: 0.7454\n",
            "Epoch 17/20\n",
            "40000/40000 [==============================] - 463s 12ms/step - loss: 0.3490 - acc: 0.8478 - val_loss: 0.5843 - val_acc: 0.7471\n",
            "Epoch 18/20\n",
            "40000/40000 [==============================] - 467s 12ms/step - loss: 0.3362 - acc: 0.8541 - val_loss: 0.5871 - val_acc: 0.7460\n",
            "Epoch 19/20\n",
            "40000/40000 [==============================] - 461s 12ms/step - loss: 0.3300 - acc: 0.8571 - val_loss: 0.6100 - val_acc: 0.7450\n",
            "Epoch 20/20\n",
            "40000/40000 [==============================] - 458s 11ms/step - loss: 0.3152 - acc: 0.8627 - val_loss: 0.6124 - val_acc: 0.7503\n",
            "Accuracy: 75.03%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czOGc1RWnoE0",
        "colab_type": "text"
      },
      "source": [
        "## Doc2Vec with Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKd2HqQJnrUM",
        "colab_type": "code",
        "outputId": "428e99cc-ffca-4e19-df19-2192af559cc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = drug[['pre_review', 'pre_rating']]\n",
        "df.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pre_review</th>\n",
              "      <th>pre_rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[['took',, 'bactrim',, 'days',, 'cleared',, '...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[['prescribed',, 'medicine',, 'sinus',, 'flu'...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[['pill',, 'three',, 'months',, 'tri',, 'spin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[['tried',, 'belviq',, 'two',, 'weeks',, 'gre...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[['medicine',, 'stops',, 'physical',, 'effect...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          pre_review  pre_rating\n",
              "0  [[['took',, 'bactrim',, 'days',, 'cleared',, '...           0\n",
              "1  [[['prescribed',, 'medicine',, 'sinus',, 'flu'...           0\n",
              "2  [[['pill',, 'three',, 'months',, 'tri',, 'spin...           0\n",
              "3  [[['tried',, 'belviq',, 'two',, 'weeks',, 'gre...           0\n",
              "4  [[['medicine',, 'stops',, 'physical',, 'effect...           0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhCBUOcen2-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## add tags to each review\n",
        "\n",
        "import gensim\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
        "\n",
        "train_tagged = train.apply(\n",
        "    lambda r: TaggedDocument(words=(r['pre_review']), tags=[r.pre_rating]), axis=1)\n",
        "test_tagged = test.apply(\n",
        "    lambda r: TaggedDocument(words=(r['pre_review']), tags=[r.pre_rating]), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYjqZdykoDhj",
        "colab_type": "code",
        "outputId": "068a3afe-1965-4b71-d81d-3b694ff9509f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_tagged[38094]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaggedDocument(words=\"[['taken', 'three', 'days', 'hot', 'flashes', 'stopped', 'within', 'hrs', 'fantastic']]\", tags=[10.0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX8t1UgAoZWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJYVQSKVpL76",
        "colab_type": "text"
      },
      "source": [
        "If dm=0, distributed bag of words (PV-DBOW) is used; if dm=1,‘distributed memory’ (PV-DM) is used.\n",
        "\n",
        "300- dimensional feature vectors.\n",
        "\n",
        "min_count=2, ignores all words with total frequency lower than this.\n",
        "\n",
        "negative=5, specifies how many “noise words” should be drawn.\n",
        "\n",
        "hs=0, and negative is non-zero, negative sampling will be used.\n",
        "\n",
        "sample=0, the threshold for configuring which higher-frequency words are randomly down sampled.\n",
        "\n",
        "workers=cores, use these many worker threads to train the model (=faster training with multicore machines)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgNIHj6DpArP",
        "colab_type": "code",
        "outputId": "578e7d44-3ab4-48f4-8e4b-c2cd8252f25e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from gensim.models import Doc2Vec\n",
        "from tqdm import tqdm\n",
        "\n",
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
        "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 35000/35000 [00:00<00:00, 1612573.63it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le0vM3Cdpbnd",
        "colab_type": "code",
        "outputId": "9f5629ee-070c-4903-9a6c-cacdffbfc68a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "from sklearn import utils\n",
        "\n",
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 35000/35000 [00:00<00:00, 1983685.21it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2463842.10it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2076681.85it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2556032.94it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2579477.43it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 3029378.24it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2563934.61it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 1838432.08it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2942368.32it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 1914458.01it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2016824.75it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2525776.23it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2326439.20it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2325481.01it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2688065.63it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2854655.13it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2278876.09it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 3007778.39it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 3069152.64it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2247407.23it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2949758.68it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2847512.12it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2942309.34it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2889776.38it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2021880.29it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2006103.56it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2538837.12it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2859659.88it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 3021957.26it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 1823157.48it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnsiQL82pr7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vec_for_learning(model, tagged_docs):\n",
        "    sents = tagged_docs.values\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
        "    return targets, regressors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6ItLUKtpvIU",
        "colab_type": "code",
        "outputId": "f702511f-fb15-47c3-851a-abda68f00ddc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
        "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing accuracy 0.5072\n",
            "Testing F1 score: 0.5071156040214638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGZ5FB2iusEw",
        "colab_type": "code",
        "outputId": "638e17f9-41a8-4def-e349-4efab134b78e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
        "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 35000/35000 [00:00<00:00, 1504598.23it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myC2t4emvem8",
        "colab_type": "code",
        "outputId": "72cee148-5ecf-436e-ee8e-bdca15721ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "for epoch in range(30):\n",
        "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
        "    model_dmm.alpha -= 0.002\n",
        "    model_dmm.min_alpha = model_dmm.alpha"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 35000/35000 [00:00<00:00, 1596856.77it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2593788.36it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2583790.48it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 3003839.50it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 1895351.25it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2174792.08it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2980360.57it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 3006423.23it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2053787.74it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 1857460.05it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2288289.56it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2684133.69it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2363634.96it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2784903.91it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 1716886.23it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2238804.35it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2760811.69it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2732598.19it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2063980.88it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 1692362.96it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2066566.81it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2673720.79it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 1866932.55it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2288289.56it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2212719.16it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2319418.57it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2095415.80it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2598931.40it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 2917747.70it/s]\n",
            "100%|██████████| 35000/35000 [00:00<00:00, 1577110.93it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97fHlDo4sj-m",
        "colab_type": "code",
        "outputId": "928cfc07-95c3-42a3-cf60-b92e73ffd010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install testfixtures\n",
        "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
        "\n",
        "new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting testfixtures\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/ac/a2d5b0e8f25696b292b52fa4f54e36f51db83c5ad07aca4883501b198015/testfixtures-6.10.3-py2.py3-none-any.whl (86kB)\n",
            "\r\u001b[K     |███▊                            | 10kB 18.5MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 71kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: testfixtures\n",
            "Successfully installed testfixtures-6.10.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyTmQNgJunIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vectors(model, tagged_docs):\n",
        "    sents = tagged_docs.values\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
        "    return targets, regressors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LoYFs22vaFe",
        "colab_type": "code",
        "outputId": "1d751fb2-cbf6-4032-a5a2-14a722af2cb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "y_train, X_train = get_vectors(new_model, train_tagged)\n",
        "y_test, X_test = get_vectors(new_model, test_tagged)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_test)\n",
        "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing accuracy 0.5726666666666667\n",
            "Testing F1 score: 0.5726432657541131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASwSmCkevlen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}